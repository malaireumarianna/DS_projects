# -*- coding: utf-8 -*-
"""Copy of linregression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UsIFmpGykgZmX7SfgxAchfsepEW4fPYB
"""

!git clone https://github.com/malaireumarianna/DS_projects.git

!pip install ydata_profiling

"""## 1. EDA:
Below will be represented some univariate and multivariate analysis of our dataset.
"""

import pandas as pd


air_data = pd.read_csv("/content/DS_projects/LinRegression/AirQualityUCI.csv",  sep=';')

air_data.head(5)

air_data = air_data.drop(columns=['Unnamed: 15',	'Unnamed: 16'])

print(air_data.isna().sum())

"""Drop entries with NAN."""

air_data = air_data.dropna()

print("Dataset Overview:\n", air_data.info())

"""AS we see from above, there are few numerical columns that hold object type, so transform them to numeric.

"""

# Replace commas with dots in the relevant columns
air_data['CO(GT)'] = air_data['CO(GT)'].str.replace(',', '.')
air_data['C6H6(GT)'] = air_data['C6H6(GT)'].str.replace(',', '.')
air_data['T'] = air_data['T'].str.replace(',', '.')
air_data['AH'] = air_data['AH'].str.replace(',', '.')
air_data['RH'] = air_data['RH'].str.replace(',', '.')


air_data['CO(GT)'] = pd.to_numeric(air_data['CO(GT)'])
air_data['C6H6(GT)'] = pd.to_numeric(air_data['C6H6(GT)'])
air_data['T'] = pd.to_numeric(air_data['T'])
air_data['AH'] = pd.to_numeric(air_data['AH'])
air_data['RH'] = pd.to_numeric(air_data['RH'])

print("Descriptive Statistics:")
print(air_data.describe())

"""From descriptive statistics above we can see that variables have a wide range of mean values. For example, C6H6(GT) has a mean of 1.86, while PT08.S4(NO2) has a much higher mean of 1391.48.
Standard deviations are also high for many variables, indicating significant variability. So the best choice would be to scale the features for further prediction of target.

"""





"""Primarily, a we can see from univariate analysis below, there are plenty of negative values in defferent features.

For example, "CO(GT)" has negative	1683 entries out of 9357, while "NMHC(GT)" has 	8443 negative values, which indicates missing values. Correlation of "NMHC(GT)" is almost zero, due to lack of data, so this column is not representative at all, it will be reasonable to drop it out of dataset.

Other features, also contain negative (missing) values, and they often have same value of 366 negative	values, which might mean that we have completelly irrepresetative rows, so it might be resonable to drop rows where all features result in -200 value.

For multivariate analysis, from corretation matrix we see potential multicolineariry.
There are features like "PT08.S1(CO)", 	"PT08.S2(NMHC)", 	"PT08.S4(NO2)",	"PT08.S5(O3)" that highly correlate with target, and they have quite high correlation berween each other.
So, it might be beneficial for the model to drop some of them.
"""

import ydata_profiling
profile = ydata_profiling.ProfileReport(air_data)
profile

"""Below is calculated variance inflation factor which helps to determine degree of multicolinearity in variables.
If VIF > 10 for some feature, then this feature can be represented as linear combination of others, so keeping all of them might make model struggle to assign resonable weights for prediction, it can lead to overfitting. SO it should be processed somehow.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd
import statsmodels.api as sm


# Select only independent variables
predictors = air_data.drop(columns=["C6H6(GT)", "Date",	"Time"])  # Exclude the target variable

# Add a constant for VIF calculation
predictors = sm.add_constant(predictors)

# Compute VIF for each feature
vif = pd.DataFrame()
vif["Variable"] = predictors.columns
vif["VIF"] = [variance_inflation_factor(predictors.values, i) for i in range(predictors.shape[1])]

print(vif)

"""##2. Data preparation:

For now, from the pair -"PT08.S5(O3)" - "PT08.S1(CO)" which has correaltion 0.906, I will retain "PT08.S1(CO)" as it has higher correlation with target - 0.902.

From pair - "PT08.S2(NMHC)" - "PT08.S1(CO)" which has correaltion 0.902, I will retain "PT08.S2(NMHC)" as it has higher correlation with target - 1.00.


"PT08.S5(O3)" and "PT08.S1(CO)" will be dropped from further analysis.
"""

air_data = air_data.drop(columns=["PT08.S1(CO)", "PT08.S5(O3)"])

"""Below, I drop "NMHC(GT)" as it is almost empty, and also I will drop rows that contain -200 in all attributes."""

air_data = air_data.drop(columns=["NMHC(GT)"])

# filter the rows, keeping only those where at least one value is not -200.

filtered_data = air_data[(air_data != -200).any(axis=1)]


print(filtered_data)

"""Since my filtering didn't drop any rows, there is still need to impute missing values.

Since we have basically time series data, then linear interpolation might be beneficial in this case. Or imputation by the help of highly correlated attribute to ours, but in this case both attribute can have missing values and imputation will not be possible.
So for now I will try to impute by using interpolation.
"""

# Combine Date and Time into a single datetime column
filtered_data["datetime"] = pd.to_datetime(filtered_data["Date"] + " " + filtered_data["Time"], format="%d/%m/%Y %H.%M.%S", errors="coerce")

# Sort the dataset by the datetime column in ascending order
filtered_data = filtered_data.sort_values(by="datetime", ascending=True)

import numpy as np


filtered_data = filtered_data.replace(-200, np.nan)
# Apply interpolation
filtered_data = filtered_data.interpolate(method='linear', axis=0) #column-wise operation

print(filtered_data.isnull().sum())

"""I will drop non-numerical features for further preparation of dataset for feeding to the model."""

filtered_data = filtered_data.drop(columns=['datetime', 'Date', 'Time'])

"""## 3. Prediction of C6H6(GT)

Below function calculates error metrics (MSE, RMSE, MAE, R^2) for predicted and actual values. Calculation will be performed on original scale of target. Since target value is standartized, it will be used inverse transform for further calculation.
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def calculate_error_scores(y_true, y_pred, y_scaler):


    """Returns:
    dict: Dictionary containing calculated error metrics (MSE, RMSE, MAE, R^2).
    """
    # inverse-transform the values
    if y_scaler:
        y_true = y_scaler.inverse_transform(y_true)
        y_pred = y_scaler.inverse_transform(y_pred)

    # calculate metrics
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)



    metrics = {
        "Mean Squared Error (MSE)": mse,
        "Root Mean Squared Error (RMSE)": rmse,
        "Mean Absolute Error (MAE)": mae,
        "R-Squared (R2)": r2
    }


    print("Error Metrics:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")



    return metrics

"""Below is presented fuction which plots scatter plot for predicted vs actual values of model and histogram of residuals."""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def plot_residuals(y_true, y_pred, y_scaler=None):
    """
     visualizes predictions and residuals."""


    # inverse-transform the values
    if y_scaler:
        y_true = y_scaler.inverse_transform(y_true)
        y_pred = y_scaler.inverse_transform(y_pred)


    # plot scatter plot of predicted vs actual values
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.scatter(y_true, y_pred, alpha=0.7, edgecolor='k')
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], color='red', linestyle='--')
    plt.title("Predicted vs Actual Values")
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.grid(True)

    # plot histogram of residuals
    residuals = y_true - y_pred

    plt.subplot(1, 2, 2)
    plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)
    plt.title("Histogram of Residuals")
    plt.xlabel("Residuals")
    plt.ylabel("Frequency")
    plt.grid(True)

    plt.tight_layout()
    plt.show()

"""Below is represented function which splits data into train and test sets. 20% of data is used for testing."""

from sklearn.model_selection import train_test_split


def split_data(pipeline, y_scaller, poly=False, cross_val=False):
    # separate features (X) and target (y)
    X = filtered_data.drop(columns=["C6H6(GT)"])
    y = filtered_data["C6H6(GT)"]


    X_transformed = pipeline.fit_transform(X)

    y_scaled = y_scaller.fit_transform(y.values.reshape(-1, 1))

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_scaled, test_size=0.2, random_state=42)

    if poly:
      # Some predictions will be based on data with polynomial features
      poly = pipeline.named_steps["poly"]

      transformed_df = pd.DataFrame(
          X_transformed,
          columns=poly.get_feature_names_out(X.columns))  # feature names from PolynomialFeatures

      return X_train, X_test, y_train, y_test, transformed_df

    elif cross_val:

      return X_transformed , y_scaled

    return X_train, X_test, y_train, y_test

"""The very first prediction model will be Linear regression withour regularization, performed on original scalled features.

Below is applied standad scaller to numerical features to keep data within the same value range.
"""

from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline


# pipeline for scaling
pipeline = Pipeline([

    ("scaler", StandardScaler())

])


# scaler for the target variable
y_scaler = StandardScaler()

from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test = split_data(pipeline, y_scaler)

model = LinearRegression()

# fit the data
model.fit(X_train, y_train)

# predict on the test set
y_pred_lr_scaled = model.predict(X_test)

# feature names from data
feature_names = filtered_data.drop(columns=["C6H6(GT)"]).columns

# access model coefficients
coefficients = model.coef_

if len(coefficients.shape) > 1:
    coefficients = coefficients.flatten()

# define feature importance
feature_importance = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": coefficients,
    "Absolute Importance": np.abs(coefficients)
})

# sort features by absolute importance
feature_importance = feature_importance.sort_values(by="Absolute Importance", ascending=False)


print("Feature Importance:")
print(feature_importance)

# calculate errors for unscaled target and predicted values from Linear Regression

metrics = calculate_error_scores(y_test, y_pred_lr_scaled, y_scaler)

#plot residuals

plot_residuals(y_test, y_pred_lr_scaled, y_scaler)

"""For linear regression on scaled original features, the results are promising, indicating quite good model performance.


Root Mean Squared Error (RMSE), which measures prediction accuracy in the same unit as the target variable, is resonable small - 1.1813.
Mean Absolute Error (MAE): 0.8530

R² score is close to 1, which means that model explains variability in the target variable quite well.

From the scatter plot we see that predictions closely align with the actual values, as most points lie near the diagonal red line.

Residuals are approximately normally distributed around zero, indicating that the model captures the data well.
Slight asymmetry is seen, which could mean that data contains some outliers on the right-hand side, like extremelly large values.

Probsbly in this case, log-scalling of target could help to treat these extreme values. But let's see how models will perform on polynomial features.

"""



from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

#  reusable pipeline function for regression model
def create_pipeline(regressor):
    return Pipeline([
        ("regressor", regressor)
    ])

# pipeline for polynomial feature generation and scaling
poly_pipeline = Pipeline([
    ("poly", PolynomialFeatures(degree=2, include_bias=False)),
    ("scaler", StandardScaler())
])

from sklearn.linear_model import LinearRegression


X_train, X_test, y_train, y_test, X_transformed = split_data(poly_pipeline, y_scaler, poly=True)


lin_regress_pipeline = create_pipeline(LinearRegression())

# fit the pipeline
lin_regress_pipeline.fit(X_train, y_train)

# predict on the test set
y_pred_lr_scaled = lin_regress_pipeline.predict(X_test)


# feature names
feature_names = X_transformed.columns

# access model coefficients
coefficients = lin_regress_pipeline['regressor'].coef_


if len(coefficients.shape) > 1:
    coefficients = coefficients.flatten()

#  feature importance
feature_importance = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": coefficients,
    "Absolute Importance": np.abs(coefficients)
})


feature_importance = feature_importance.sort_values(by="Absolute Importance", ascending=False)

# Display feature importance
print("Feature Importance:")
print(feature_importance)

# calculate errors for unscaled target and predicted values from Linear Regression

metrics = calculate_error_scores(y_test, y_pred_lr_scaled, y_scaler)

#plot residuals

plot_residuals(y_test, y_pred_lr_scaled, y_scaler)

"""From results above we can see that, feature "PT08.S2(NMHC)^2" has highest insfluence on prediction. Even without polynomial degree, this feature has highest absolute importance.

Errors, such as MSE, MAE are extremely low, showing accurate predictions for most observations.
RMSE of 0.1005 is significantly lower than in first experiment, and indicates basically minimal difference between target and predicted values.

R² score indicates that 99.98% of the variability in the target variable is explained by the model, which is near-perfect performance.

Scatter plot almost perfectly fits to red line.
Residuals are centered around 0 with a narrow spread, showing minimal error in predictions.

"""



"""Below will be performed Ridge and Lasso regression with grid search to find best alpha parameter on train set.

And also will be performed cross-validation on whole dataset.
"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV, cross_val_score

X, y = split_data(poly_pipeline, y_scaler, poly=False, cross_val=True)

X_train, X_test, y_train, y_test, X_transformed = split_data(poly_pipeline, y_scaler, poly=True)

ridge_pipeline = create_pipeline(Ridge())

# hyperparameter tuning for Ridge

param_grid = {
    "regressor__alpha": [0.01, 0.1, 1, 10]
}

grid_search = GridSearchCV(ridge_pipeline, param_grid, scoring="neg_mean_squared_error", cv=5)
grid_search.fit(X_train, y_train)
print("Best alpha for Ridge:", grid_search.best_params_)


best_ridge_pipeline = grid_search.best_estimator_


y_pred_ridge_scaled = best_ridge_pipeline.predict(X_test)


# access model coefficients
coefficients = best_ridge_pipeline['regressor'].coef_


if len(coefficients.shape) > 1:
    coefficients = coefficients.flatten()


feature_importance = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": coefficients,
    "Absolute Importance": np.abs(coefficients)
})


feature_importance = feature_importance.sort_values(by="Absolute Importance", ascending=False)


print("Feature Importance:")
print(feature_importance)

from sklearn.model_selection import cross_val_predict


# perform cross-validation and get predictions
y_pred_cv_scaled = cross_val_predict(best_ridge_pipeline, X, y, cv=5)

# calculate cross-validation error metrics using the provided function
cv_metrics = calculate_error_scores(y, y_pred_cv_scaled.reshape(-1, 1), y_scaler=y_scaler)

print("\nCross-Validation Error Metrics:")
for metric, value in cv_metrics.items():
    print(f"{metric}: {value:.4f}")

#plot residuals

plot_residuals(y_test, y_pred_ridge_scaled, y_scaler)



# usage with Ridge regression
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV, cross_val_score


X, y = split_data(poly_pipeline, y_scaler, poly=False, cross_val=True)

X_train, X_test, y_train, y_test, X_transformed = split_data(poly_pipeline, y_scaler, poly=True)


# hyperparameter tuning for Lasso

param_grid = {
    "regressor__alpha": [0.01, 0.1, 1, 10]
}

lasso = create_pipeline(Lasso())
grid_search = GridSearchCV(lasso, param_grid, scoring="neg_mean_squared_error", cv=5)
grid_search.fit(X_train, y_train)
print("Best alpha for Lasso:", grid_search.best_params_)

# Use the best parameters in a new pipeline
best_lasso_pipeline = grid_search.best_estimator_



y_pred_lasso_scaled = best_lasso_pipeline.predict(X_test)


# Access model coefficients
coefficients = best_lasso_pipeline['regressor'].coef_



if len(coefficients.shape) > 1:
    coefficients = coefficients.flatten()

# Create a DataFrame for feature importance
feature_importance = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": coefficients,
    "Absolute Importance": np.abs(coefficients)
})

feature_importance = feature_importance.sort_values(by="Absolute Importance", ascending=False)


print("Feature Importance:")
print(feature_importance)

from sklearn.model_selection import cross_val_predict

# perform cross-validation and get predictions
y_pred_cv_scaled = cross_val_predict(best_lasso_pipeline, X, y, cv=5)

# calculate cross-validation error metrics using the provided function
cv_metrics = calculate_error_scores(y, y_pred_cv_scaled.reshape(-1, 1), y_scaler=y_scaler)

print("\nCross-Validation Error Metrics:")
for metric, value in cv_metrics.items():
    print(f"{metric}: {value:.4f}")

#plot residuals

plot_residuals(y_test, y_pred_lasso_scaled.reshape(-1, 1), y_scaler)

"""From two experiments above, we see that Ridge keeps all features in the model but reduces the coefficients for less significant features.
The top feature is PT08.S2(NMHC)^2 with an absolute importance of 1.04.
Ridge allows smaller contributions, while Lasso sets many coefficients to 0, effectively performing feature selection.
Only 2 features (PT08.S2(NMHC)^2 and CO(GT) PT08.S2(NMHC)) have non-zero coefficients.
The most important feature remains PT08.S2(NMHC)^2 with an absolute importance of 0.98, similar to Ridge.

Ridge regression achieved slightly lower errors compared to Lasso.
And R² score of 0.9996, slightly better explained variance.


On scatter plots, Ridge and Lasso align predictions closely with the actual values in terms of diagonal line.

Ridge residuals are more narrowly distributed around 0.
Lasso residuals show a slightly wider spread, consistent with its higher RMSE and MAE.
"""
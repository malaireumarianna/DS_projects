# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ovZHiS0LgY5-qDeWpktyc5a2MgphSU5X
"""

!git clone https://github.com/malaireumarianna/DS_projects.git

!pip install ydata_profiling
!pip install missingno
!pip install pymcar

"""## 1. EDA and data preparation:
Below will be represented some univariate and multivariate analysis of our dataset.
"""

import pandas as pd


weather_data = pd.read_csv("/content/DS_projects/Classification/weatherAUS.csv",  sep=',')

weather_data.head(5)

print("Dataset Overview:\n", weather_data.info())

"""As we see, there are quite many missing values in features represented in the dataset. The highest amount of missing data is represented in Evaporation, Sunshine, Cloud9am, Cloud3pm features. Other features contain moderalety less missing values.

The imputation logic is required for further predictive analysis. So I will try to build this logic further.
"""

print(weather_data.isna().sum())

sns.heatmap(weather_data.isna());
plt.title('Visualization of NAs in dataset (white - missing values)\n');

print('Maximum number of missing values = number of columns in df = {}'.format(len(weather_data.columns)))

"""Since we have 145460 rows of data and 23 columns, I will drop rows which contain missing values for at least 18 features. I think, even with accurate imputation logic leaving only 5 values which we know for sure for this particular observation will be quite biased for predicion of target"""

bad_rows = weather_data[weather_data.shape[1] - weather_data.count(axis=1) > 18].index
weather_data.drop(bad_rows, axis = 0, inplace = True)
weather_data

"""It reduced num of rows to 145167.

Also, there are some missing values in target variable. Imputation here might affect the prediction drastically, so it would be safer to drop them, since we still have vast part of dataset remaining.
"""

weather_data.drop(weather_data[weather_data['RainTomorrow'].isnull()].index, axis = 0, inplace = True)

weather_data.shape

"""We have time-dependent data set. In order to perform validation in a real-world scenario, predictions are based on historical data. Sorting by location ensures that data within each location remains consistent across training and validation splits since data can contain regional dependencies.
so I will order dataset by date and location and use first 80% - 113 737 rows for training and last for validation - 28 435.
"""

weather_data['Date'] = pd.to_datetime(weather_data['Date'])


weather_data = weather_data.sort_values(by=['Date', 'Location'])

train, test = weather_data.iloc[:113737,:], weather_data.iloc[113737:,:]



print("Descriptive Statistics:")
print(weather_data.describe())

import matplotlib.pyplot as plt
import seaborn as sns


numerical_features = train.select_dtypes(include=['int64', 'float64']).columns

num_plots = len(numerical_features)
rows, cols = 8, 2
fig, axes = plt.subplots(rows, cols, figsize=(10, 15))
axes = axes.flatten()
for i, feature in enumerate(numerical_features):
    if i < len(numerical_features):

        sns.histplot(train[feature], kde=True, bins=100, color='blue', ax=axes[i])
        axes[i].set_title(f'Distribution of {feature}')
        axes[i].set_ylabel('')
    else:
        axes[i].axis('off')
plt.tight_layout()
plt.show()

"""From statistics and charts above, we see that there are some features with nearly normal distribution, but still some of them are right-skewed like Rainfall and Evaporation, so to fill NAs with _medians_ would be the best strategy. But I would like to analize the correlation between features and try to fill missing values by using iterative imputer.

"""

# Plot value counts for categorical features
categorical_features = train.select_dtypes(include=['object']).columns

categorical_features = categorical_features.difference(['Date', 'Location'])

print(categorical_features)

num_plots = len(categorical_features)
rows, cols = 3, 2
fig, axes = plt.subplots(rows, cols, figsize=(10, 15))
axes = axes.flatten()
for i, feature in enumerate(categorical_features):
    if i < len(categorical_features):
        sns.countplot(y=train[feature], order=weather_data[feature].value_counts().index, ax=axes[i])
        axes[i].set_title(f'Distribution of {feature}')
        axes[i].set_ylabel('')
    else:
        axes[i].axis('off')
plt.tight_layout()
plt.show()

"""RainToday and RainTomorrow:
Both are imbalanced, with "No" being the majority class.
Highlights the need for techniques like class weighting or resampling during model training
"""

import matplotlib.pyplot as plt
import seaborn as sns


numerical_features = weather_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(12, 8))
corr_matrix = weather_data[numerical_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()



"""## __Filling NAN's__

From correlation matrix above we see some highly correlated features, which will help to impute each other, but I still prefer to use imputation based on all numerical features as even weakly correlated features might carry useful information for imputation. Especially for those features, which doesn't have high correlation with others, preserving all features for imputation will give more meaningful results.

The imputer is fit only on train data, to prevent data leackage.
"""

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import pandas as pd



columns_to_impute = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',
                      'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',
                      'Humidity9am', 'Humidity3pm',
                      'Pressure9am', 'Pressure3pm',
                      'Cloud9am', 'Cloud3pm',
                      'Temp9am', 'Temp3pm']


iterative_imputer = IterativeImputer(max_iter=10, random_state=42)
train[columns_to_impute] = iterative_imputer.fit_transform(train[columns_to_impute])

test[columns_to_impute] = iterative_imputer.transform(test[columns_to_impute])


print("Missing values in train set after imputation:\n", train[columns_to_impute].isna().sum())
print("Missing values in test set after imputation:\n", test[columns_to_impute].isna().sum())



"""For categorical features, data will be grouped on location, because weather data can be vary from location and is sensitive to it. the mode will be defined on trained set and applied to the whole data. If mode based on location is not found, then use global mode from train data."""

categorical_features = ['RainToday', 'RainTomorrow', 'WindDir9am', 'WindDir3pm', 'WindGustDir']

grouped_mode_dict = {}
for feature in categorical_features:

    global_mode = train[feature].mode()[0]
    grouped_mode = train.groupby('Location')[feature].agg(lambda x: x.mode()[0] if not x.mode().empty else global_mode)
    grouped_mode_dict[feature] = grouped_mode


    train[feature] = train.apply(
        lambda row: grouped_mode[row['Location']] if pd.isna(row[feature]) else row[feature],
        axis=1
    )


for feature in categorical_features:
    global_mode = train[feature].mode()[0]
    test[feature] = test.apply(
        lambda row: grouped_mode_dict[feature][row['Location']]
        if pd.isna(row[feature]) and row['Location'] in grouped_mode_dict[feature]
        else global_mode if pd.isna(row[feature])
        else row[feature],
        axis=1
    )

print("\nMissing values in train set after imputation:\n", train[categorical_features].isna().sum())
print("\nMissing values in test set after imputation:\n", test[categorical_features].isna().sum())



"""## Dealing with outliers and data transformation"""

import matplotlib.pyplot as plt


plt.figure(figsize=(12, 8))
train.select_dtypes(include=['float64', 'int64']).boxplot()
plt.title('Boxplot for Numerical Features (Outliers Highlighted)', fontsize=16)
plt.xticks(rotation=45)
plt.show()

"""As we see there are some outliers in numerical features. Especially are visible in Rainfall, WindGustSpeed, Evaporation, etc. I will use interquartile range to drop them. Interquartile range is calculated on train data."""

import pandas as pd
import matplotlib.pyplot as plt


numerical_features = train.select_dtypes(include=['float64', 'int64']).columns


iqr_bounds = {}
for feature in numerical_features:
    Q1 = train[feature].quantile(0.25)
    Q3 = train[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR


    iqr_bounds[feature] = (lower_bound, upper_bound)
    train = train[(train[feature] >= lower_bound) & (train[feature] <= upper_bound)]

for feature, (lower_bound, upper_bound) in iqr_bounds.items():

    test = test[(test[feature] >= lower_bound) & (test[feature] <= upper_bound)]

"""Next, let's encode categorical features.
For RainToday anf RainTomorrow yes will be exchanged with 1 and no - with 0.
"""

for feature in ['RainToday', 'RainTomorrow']:
    train[feature] = train[feature].map({'Yes': 1, 'No': 0})
    test[feature] = test[feature].map({'Yes': 1, 'No': 0})

print(train[['RainToday', 'RainTomorrow']].head())
print(test[['RainToday', 'RainTomorrow']].head())

"""The rest of categorical features are encoded by using one hot encoding, since these features don't have order."""

categorical_features = ['WindDir9am', 'WindDir3pm', 'WindGustDir', 'Location']

train = pd.get_dummies(train, columns=categorical_features, drop_first=True)
test = pd.get_dummies(test, columns=categorical_features, drop_first=True)

# align columns in train and test to ensure consistency, in case test set has categories not present in train
train, test = train.align(test, join='left', axis=1)

test.fillna(0, inplace=True)

train.drop(columns=['Date'], inplace=True)
test.drop(columns=['Date'], inplace=True)

"""Since some of numerical features show a bell-shaped distribution, which is approximately normal, then it would be resonable to use
StandardScaler, for skewed data it might be not that useful, but since I dropped outliers, then this scaller might still be beneficial.
"""

from sklearn.preprocessing import StandardScaler


numerical_features = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',
                      'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',
                      'Humidity9am', 'Humidity3pm',
                      'Pressure9am', 'Pressure3pm',
                      'Cloud9am', 'Cloud3pm',
                      'Temp9am', 'Temp3pm']

scaler = StandardScaler()


train[numerical_features] = scaler.fit_transform(train[numerical_features])
test[numerical_features] = scaler.transform(test[numerical_features])



print(train.head())

print(test.head())

train_X, train_y = train.drop('RainTomorrow', axis = 1), train['RainTomorrow']

test_X, test_y = test.drop('RainTomorrow', axis = 1), test['RainTomorrow']

train_y

"""##Training models"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

#  evaluate metrics
def evaluate_metrics(y_true, y_pred, y_prob):
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred))
    print("Recall:", recall_score(y_true, y_pred))
    print("F1 Score:", f1_score(y_true, y_pred))
    print("ROC AUC:", roc_auc_score(y_true, y_prob))

"""To deal with imbalanced target data, I will use oversampling for logistic regression."""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import numpy as np
from imblearn.over_sampling import SMOTE


smote = SMOTE(random_state=42)
train_X_resampled, train_y_resampled = smote.fit_resample(train_X, train_y)

logistic_model = LogisticRegression(max_iter=300, random_state=42)
logistic_model.fit(train_X_resampled, train_y_resampled)


y_pred = logistic_model.predict(test_X)
y_prob = logistic_model.predict_proba(test_X)[:, 1]
evaluate_metrics(test_y, y_pred, y_prob)


param_grid = {'C': [0.01, 0.1, 1]}
grid = GridSearchCV(LogisticRegression(max_iter=300, random_state=42), param_grid, cv=5, scoring='roc_auc')
grid.fit(train_X_resampled, train_y_resampled)
print("Best params", grid.best_params_)


coefficients = logistic_model.coef_[0]

feature_importance = pd.DataFrame({
    'Feature': train_X_resampled.columns,
    'Coefficient': logistic_model.coef_[0]
})

feature_importance['AbsCoefficient'] = feature_importance['Coefficient'].abs()
top_features = feature_importance.sort_values(by='AbsCoefficient', ascending=False).head(20)


plt.figure(figsize=(10, 6))
plt.barh(top_features['Feature'], top_features['Coefficient'])
plt.title("Feature Importance")
plt.show()


thresholds = np.arange(0.1, 1, 0.1)
f1_scores = []
for threshold in thresholds:
    y_pred_threshold = (y_prob > threshold).astype(int)
    f1_scores.append(f1_score(test_y, y_pred_threshold))

plt.figure(figsize=(10, 6))
plt.plot(thresholds, f1_scores, marker='o')
plt.title("F1 Score vs. Threshold")
plt.xlabel("Threshold")
plt.ylabel("F1 Score")
plt.show()

print("Best threshold for F1 Score:", thresholds[np.argmax(f1_scores)])

"""
LR model correctly predicts the target class 84.9% of the time, but it indicates moderate confidence in the model's positive predictions based on precision outcome.

The model captures around 50.77% of actual positive cases. This is quite low and suggests the model may be missing some rainy days.

A good AUC score indicates the model is effective at distinguishing between the two classes overall.

From feature importance we see that location-based features are crucial for prediction.The dominance of location-based features suggests the model captures geographic variations effectively but relies less on meteorological factors.

The optimal threshold of 0.5 balances precision and recall effectively."""

from sklearn.neighbors import KNeighborsClassifier
from imblearn.over_sampling import SMOTE


smote = SMOTE(random_state=42)
train_X_resampled, train_y_resampled = smote.fit_resample(train_X, train_y)


k_values = range(1, 10)
f1_scores = []

for k in k_values:
    knn_model = KNeighborsClassifier(n_neighbors=k)
    knn_model.fit(train_X_resampled, train_y_resampled)
    y_pred = knn_model.predict(test_X)
    f1_scores.append(f1_score(test_y, y_pred))

plt.figure(figsize=(10, 6))
plt.plot(k_values, f1_scores, marker='o')
plt.title("F1 Score vs. Number of Neighbors (K)")
plt.xlabel("Number of Neighbors (K)")
plt.ylabel("F1 Score")
plt.show()



optimal_k = k_values[np.argmax(f1_scores)]
knn_model = KNeighborsClassifier(n_neighbors=optimal_k)
knn_model.fit(train_X, train_y)
y_pred = knn_model.predict(test_X)
y_prob = knn_model.predict_proba(test_X)[:, 1]
evaluate_metrics(test_y, y_pred, y_prob)

"""Since it takes long to fit the model for different k value, I used only up to 9(including) nearest heighbours for analysis.

The model performs well for correctly predicting both classes in terms of accuracy.

When the model predicts rain, it is correct nearly 69% of the time. This shows good performance for positive predictions, but it struggles to detect the minority class, if looking on recall.
The model shows a poor balance between precision and recall.
ROC AUC looks as good score, showing the model is effective at ranking predictions.
The best number of neighbours for prediction in terms of F1 score is 9, but since it's upper limit, it might resonable to use bigger k for experimentation, if training time is not the goal for improvement.

For Bayes classifier I will use undersampling to balance the target.
"""

from sklearn.naive_bayes import GaussianNB
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import make_scorer, roc_auc_score

undersampler = RandomUnderSampler(random_state=42)
train_X_resampled, train_y_resampled = undersampler.fit_resample(train_X, train_y)


# parameter grid for var_smoothing
param_grid = {'var_smoothing': np.logspace(-9, -1, 10)}


nb_model = GridSearchCV(
    estimator=GaussianNB(),
    param_grid=param_grid,
    scoring=make_scorer(roc_auc_score, needs_proba=True),
    cv=5
)

nb_model.fit(train_X_resampled, train_y_resampled)

print("Best Params", nb_model.best_params_)
y_pred = nb_model.best_estimator_.predict(test_X)
y_prob = nb_model.best_estimator_.predict_proba(test_X)[:, 1]


evaluate_metrics(test_y, y_pred, y_prob)

"""In terms of accuracy Naive Bayes predicts the target worse than KNN and LR, but there is still difference how inbalance was handeled for these approaches, which might affect the prediction.
The low precision indicates a high number of false positives predicted.
The F1 score indicates moderate performance. While recall is strong, the low precision pulls down the F1 score.

A solid ROC AUC score shows that the model is relatively effective at ranking predictions.

For SVC will be use class weighting
"""

from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score

param_grid = {'C': [0.001, 0.1, 1]}


svm_grid = GridSearchCV(
    LinearSVC(class_weight='balanced', random_state=42),
    param_grid,
    cv=5,
    scoring='roc_auc'
)
svm_grid.fit(train_X, train_y)
print("Best params:", svm_grid.best_params_)


svm_model = svm_grid.best_estimator_

# calibrate LinearSVC to get probabilities
calibrated_svm = CalibratedClassifierCV(svm_model, cv="prefit")
calibrated_svm.fit(train_X, train_y)


y_pred = calibrated_svm.predict(test_X)
y_prob = calibrated_svm.predict_proba(test_X)[:, 1]


evaluate_metrics(test_y, y_pred, y_prob)

"""The model achieves high overall accuracy, the highest for all other models.

The model predicts rain with a precision of 68.68%, indicating that when it predicts rain, it is correct in most cases, but this is still moderate number of false positives.
Recall is still small, same as for other models.
While precision is relatively high, the lower recall pulls the F1 score down.

A strong ROC AUC indicates that the model ranks predictions effectively and is capable of distinguishing between rainy and non-rainy days.
"""

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


models = {
    'Logistic Regression': logistic_model,
    'KNN': knn_model,
    'Naive Bayes': nb_model,
    'SVM': calibrated_svm
}


plt.figure(figsize=(12, 8))
for name, model in models.items():

    y_prob = model.predict_proba(test_X)[:, 1]
    fpr, tpr, _ = roc_curve(test_y, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC: {roc_auc:.2f})')

plt.title("ROC Curve for All Models", fontsize=16)
plt.xlabel("False Positive Rate", fontsize=14)
plt.ylabel("True Positive Rate", fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

"""The ROC curves show that all models perform significantly better than random guessing AUC = 0.5.
SVM outperforms other models across almost all thresholds, reflecting its robust performance.
Logistic Regression and KNN are closely matched, but Logistic Regression slightly outperforms KNN.
Naive Bayes starts strong but eventually indicates weaker performance.
"""

def compare_models(models, test_X, test_y):
    results = {}
    for name, model in models.items():
        y_pred = model.predict(test_X)
        y_prob = model.predict_proba(test_X)[:, 1]
        results[name] = {
            'Accuracy': accuracy_score(test_y, y_pred),
            'F1 Score': f1_score(test_y, y_pred),
            'ROC AUC': roc_auc_score(test_y, y_prob)
        }
    return pd.DataFrame(results).T

model_evaluation = compare_models(models, test_X, test_y)

print("Model Evaluation:")
print(model_evaluation)


print("\nComparison between Logistic Regression and KNN:")
logistic_vs_knn = model_evaluation.loc[['Logistic Regression', 'KNN']]
print(logistic_vs_knn)


print("\nComparison between SVM and NBayes:")
svc_vs_nb = model_evaluation.loc[['SVM', 'Naive Bayes']]
print(svc_vs_nb)

"""SVM achieved
highest accuracy and is the most reliable for overall prediction.It also holds best ROC AUC, and is considered as the strongest overall model.

Logistic Regression holds best F1 Score indicating balance between precision and recall, making it effective for imbalanced data.

It also has strong ROC AUC  and high Accuracy, which make it competitively close to SVM, making it a strong baseline model.
KNN holds high accuracy, but struggles with imbalanced classes.

It also has moderate ROC AUC, which indicates that it's still effective but slightly weaker compared to Logistic Regression and SVM.

Naive Bayes has owest accuracy, which means that it struggles with overall classification performance, and it holds lowest F1 score  indicating pure balance between precision and recall.

Logistic Regression vs. KNN


Logistic Regression:
Outperforms KNN in F1 Score and ROC AUC , making it better model for imbalanced data.

KNN:
Higher accuracy but lower F1 Score indicates that KNN is over-prioritizing the majority class and shows poor recall.

SVM vs. Naive Bayes

SVM:
Significantly higher Accuracy  and ROC AUC compared to Naive Bayes.
Slightly better F1 Score, indicating it balances precision and recall better.
Naive Bayes:
Shows strong recall but suffers from low precision, leading to weaker overall metrics.
"""

















